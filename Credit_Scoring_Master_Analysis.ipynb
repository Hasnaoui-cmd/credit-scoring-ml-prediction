{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üéì Credit Scoring: Master Class Analysis\n",
                "\n",
                "**Author**: Senior Data Scientist  \n",
                "**Date**: January 2026  \n",
                "**Objective**: Build and compare ML models to predict credit default risk\n",
                "\n",
                "---\n",
                "\n",
                "## Table of Contents\n",
                "1. [Introduction & Data Loading](#section1)\n",
                "2. [Exploratory Data Analysis](#section2)\n",
                "3. [Data Preprocessing Pipeline](#section3)\n",
                "4. [Model Training & Comparison](#section4)\n",
                "5. [Advanced Feature Analysis](#section5)\n",
                "6. [Threshold Optimization](#section6)\n",
                "7. [Conclusions](#section7)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a id='section1'></a>\n",
                "## 1. Introduction & Data Loading\n",
                "\n",
                "### Why This Matters\n",
                "Credit scoring is critical for financial institutions to assess the risk of lending. A good model can:\n",
                "- Minimize loan defaults (reduce financial loss)\n",
                "- Approve creditworthy applicants (maximize revenue)\n",
                "- Ensure fair lending practices\n",
                "\n",
                "### Dataset Overview\n",
                "We'll work with a comprehensive credit risk dataset containing:\n",
                "- **Demographic features**: age, income, employment length\n",
                "- **Loan characteristics**: amount, interest rate, intent, grade\n",
                "- **Credit history**: default history, credit history length\n",
                "- **Target variable**: `loan_status` (0 = paid, 1 = defaulted)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import essential libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# ML libraries\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
                "from sklearn.impute import SimpleImputer\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.naive_bayes import GaussianNB\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.metrics import (\n",
                "    accuracy_score, precision_score, recall_score, f1_score,\n",
                "    roc_curve, auc, confusion_matrix, classification_report\n",
                ")\n",
                "from imblearn.over_sampling import SMOTE\n",
                "\n",
                "# Visualization libraries\n",
                "try:\n",
                "    import plotly.express as px\n",
                "    import plotly.graph_objects as go\n",
                "    PLOTLY_AVAILABLE = True\n",
                "except ImportError:\n",
                "    PLOTLY_AVAILABLE = False\n",
                "    print(\"‚ö†Ô∏è Plotly not available, using matplotlib for all visualizations\")\n",
                "\n",
                "# Set visualization style\n",
                "plt.style.use('seaborn-v0_8-darkgrid')\n",
                "sns.set_palette('husl')\n",
                "\n",
                "print(\"‚úÖ All libraries imported successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the dataset\n",
                "df = pd.read_csv('credit_risk_dataset.csv')\n",
                "\n",
                "print(\"üìä Dataset loaded successfully!\")\n",
                "print(f\"Shape: {df.shape}\")\n",
                "print(f\"\\nFirst few rows:\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ‚úÖ ASSERTION: Verify data was loaded correctly\n",
                "assert df.shape[0] > 0, \"Dataset is empty!\"\n",
                "assert 'loan_status' in df.columns, \"Target variable 'loan_status' not found!\"\n",
                "\n",
                "print(\"‚úÖ Data integrity checks passed!\")\n",
                "print(f\"\\nDataset Info:\")\n",
                "df.info()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a id='section2'></a>\n",
                "## 2. Exploratory Data Analysis (EDA)\n",
                "\n",
                "### Why EDA Matters\n",
                "Before building models, we must understand our data:\n",
                "- **Missing values**: Can bias our model if not handled properly\n",
                "- **Class imbalance**: May require special techniques (SMOTE)\n",
                "- **Outliers**: Can skew model performance\n",
                "- **Feature distributions**: Help us choose appropriate preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check for missing values\n",
                "print(\"üîç Missing Values Analysis:\")\n",
                "missing_data = df.isnull().sum()\n",
                "missing_pct = (missing_data / len(df)) * 100\n",
                "missing_df = pd.DataFrame({\n",
                "    'Missing Count': missing_data,\n",
                "    'Percentage': missing_pct\n",
                "})\n",
                "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
                "\n",
                "if len(missing_df) > 0:\n",
                "    print(missing_df)\n",
                "else:\n",
                "    print(\"‚úÖ No missing values found!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Target variable distribution\n",
                "print(\"üéØ Target Variable Distribution:\")\n",
                "target_counts = df['loan_status'].value_counts()\n",
                "print(target_counts)\n",
                "print(f\"\\nClass Imbalance Ratio: {target_counts[1] / target_counts[0]:.2f}\")\n",
                "\n",
                "# Visualize\n",
                "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
                "\n",
                "# Count plot\n",
                "ax[0].bar(['No Default (0)', 'Default (1)'], target_counts.values, color=['#2ecc71', '#e74c3c'])\n",
                "ax[0].set_ylabel('Count', fontsize=12, fontweight='bold')\n",
                "ax[0].set_title('Loan Status Distribution', fontsize=14, fontweight='bold')\n",
                "ax[0].grid(alpha=0.3)\n",
                "\n",
                "# Pie chart\n",
                "ax[1].pie(target_counts.values, labels=['No Default', 'Default'], autopct='%1.1f%%',\n",
                "          colors=['#2ecc71', '#e74c3c'], startangle=90)\n",
                "ax[1].set_title('Default Rate', fontsize=14, fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nüí° Insight: The dataset shows class imbalance. We'll use SMOTE to balance the training set.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Statistical summary of numerical features\n",
                "print(\"üìà Statistical Summary of Numerical Features:\")\n",
                "df.describe().T"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üîç Correlation Analysis\n",
                "\n",
                "**Why correlations matter:**\n",
                "- Help identify which features are most related to default risk\n",
                "- Reveal multicollinearity (features that are too similar)\n",
                "- Guide feature selection and engineering"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Correlation heatmap\n",
                "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
                "correlation_matrix = df[numerical_cols].corr()\n",
                "\n",
                "plt.figure(figsize=(12, 8))\n",
                "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
                "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
                "plt.title('Feature Correlation Heatmap', fontsize=16, fontweight='bold', pad=20)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nüí° Key Insights:\")\n",
                "print(\"- Look for features with high correlation to 'loan_status' (our target)\")\n",
                "print(\"- Features with very high correlation to each other may cause multicollinearity\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a id='section3'></a>\n",
                "## 3. Data Preprocessing Pipeline\n",
                "\n",
                "### The Importance of Proper Preprocessing\n",
                "\n",
                "**Why each step matters:**\n",
                "\n",
                "1. **Missing Value Imputation**: Prevents data loss and model errors\n",
                "2. **Outlier Detection (IQR)**: Removes extreme values that can skew predictions\n",
                "3. **Feature Encoding**: Converts categorical variables to numerical format\n",
                "4. **Normalization**: Ensures features are on the same scale (critical for distance-based algorithms)\n",
                "5. **Class Balancing (SMOTE)**: Prevents model bias toward the majority class"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 1: Handle missing values\n",
                "print(\"üîß Step 1: Handling Missing Values\")\n",
                "print(\"Strategy: Median imputation for numerical, mode for categorical\\n\")\n",
                "\n",
                "df_clean = df.copy()\n",
                "\n",
                "# Identify column types\n",
                "numerical_features = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
                "categorical_features = df_clean.select_dtypes(include=['object']).columns.tolist()\n",
                "\n",
                "# Remove target from numerical features\n",
                "if 'loan_status' in numerical_features:\n",
                "    numerical_features.remove('loan_status')\n",
                "\n",
                "# Impute numerical columns\n",
                "if numerical_features:\n",
                "    num_imputer = SimpleImputer(strategy='median')\n",
                "    df_clean[numerical_features] = num_imputer.fit_transform(df_clean[numerical_features])\n",
                "\n",
                "# Impute categorical columns\n",
                "if categorical_features:\n",
                "    cat_imputer = SimpleImputer(strategy='most_frequent')\n",
                "    df_clean[categorical_features] = cat_imputer.fit_transform(df_clean[categorical_features])\n",
                "\n",
                "print(f\"‚úÖ Missing values handled: {df_clean.isnull().sum().sum()} remaining\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 2: Outlier Detection using IQR (Interquartile Range)\n",
                "print(\"üîß Step 2: Outlier Detection & Removal (IQR Method)\")\n",
                "print(\"Why IQR? It's robust to extreme values and works well for skewed distributions\\n\")\n",
                "\n",
                "df_no_outliers = df_clean.copy()\n",
                "outliers_removed = 0\n",
                "\n",
                "for col in numerical_features:\n",
                "    Q1 = df_no_outliers[col].quantile(0.25)\n",
                "    Q3 = df_no_outliers[col].quantile(0.75)\n",
                "    IQR = Q3 - Q1\n",
                "    \n",
                "    # Define outlier bounds\n",
                "    lower_bound = Q1 - 1.5 * IQR\n",
                "    upper_bound = Q3 + 1.5 * IQR\n",
                "    \n",
                "    # Count outliers before removal\n",
                "    outliers_count = ((df_no_outliers[col] < lower_bound) | (df_no_outliers[col] > upper_bound)).sum()\n",
                "    outliers_removed += outliers_count\n",
                "    \n",
                "    # Remove outliers\n",
                "    df_no_outliers = df_no_outliers[\n",
                "        (df_no_outliers[col] >= lower_bound) & (df_no_outliers[col] <= upper_bound)\n",
                "    ]\n",
                "\n",
                "print(f\"üìâ Original dataset size: {len(df_clean)}\")\n",
                "print(f\"üìâ After outlier removal: {len(df_no_outliers)}\")\n",
                "print(f\"üìä Total outliers removed: {len(df_clean) - len(df_no_outliers)} rows\")\n",
                "print(f\"‚úÖ Data quality improved!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 3: Split features and target\n",
                "print(\"üîß Step 3: Separating Features and Target\")\n",
                "\n",
                "X = df_no_outliers.drop('loan_status', axis=1)\n",
                "y = df_no_outliers['loan_status']\n",
                "\n",
                "print(f\"‚úÖ Features shape: {X.shape}\")\n",
                "print(f\"‚úÖ Target shape: {y.shape}\")\n",
                "\n",
                "# ‚úÖ ASSERTION: Verify shapes match\n",
                "assert X.shape[0] == y.shape[0], \"Feature and target sizes don't match!\"\n",
                "print(\"‚úÖ Shape verification passed!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 4: Train-Test Split\n",
                "print(\"üîß Step 4: Train-Test Split (80-20 with stratification)\")\n",
                "print(\"Why stratify? Ensures both sets have similar class distributions\\n\")\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.2, random_state=42, stratify=y\n",
                ")\n",
                "\n",
                "print(f\"üìä Training set: {X_train.shape[0]} samples\")\n",
                "print(f\"üìä Test set: {X_test.shape[0]} samples\")\n",
                "\n",
                "# ‚úÖ ASSERTION: Verify split sizes\n",
                "assert X_train.shape[0] == y_train.shape[0], \"Train features and labels don't match!\"\n",
                "assert X_test.shape[0] == y_test.shape[0], \"Test features and labels don't match!\"\n",
                "assert X_train.shape[0] + X_test.shape[0] == X.shape[0], \"Data lost during split!\"\n",
                "print(\"‚úÖ Split verification passed!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 5: Feature Engineering - Encoding & Scaling\n",
                "print(\"üîß Step 5: Feature Encoding & Normalization\")\n",
                "print(\"Why normalize? Models like Logistic Regression are sensitive to feature scales\\n\")\n",
                "\n",
                "# Update feature lists after outlier removal\n",
                "numerical_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
                "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
                "\n",
                "print(f\"Numerical features ({len(numerical_features)}): {numerical_features}\")\n",
                "print(f\"Categorical features ({len(categorical_features)}): {categorical_features}\\n\")\n",
                "\n",
                "# Create preprocessing pipeline\n",
                "preprocessor = ColumnTransformer(\n",
                "    transformers=[\n",
                "        ('num', StandardScaler(), numerical_features),\n",
                "        ('cat', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), \n",
                "         categorical_features)\n",
                "    ])\n",
                "\n",
                "# Fit and transform\n",
                "X_train_processed = preprocessor.fit_transform(X_train)\n",
                "X_test_processed = preprocessor.transform(X_test)\n",
                "\n",
                "print(f\"‚úÖ Processed training features shape: {X_train_processed.shape}\")\n",
                "print(f\"‚úÖ Processed test features shape: {X_test_processed.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 6: Handle Class Imbalance with SMOTE\n",
                "print(\"üîß Step 6: Balancing Classes with SMOTE\")\n",
                "print(\"Why SMOTE? Creates synthetic samples of the minority class instead of just duplicating\\n\")\n",
                "\n",
                "print(\"Before SMOTE:\")\n",
                "print(pd.Series(y_train).value_counts())\n",
                "\n",
                "smote = SMOTE(random_state=42)\n",
                "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_processed, y_train)\n",
                "\n",
                "print(\"\\nAfter SMOTE:\")\n",
                "print(pd.Series(y_train_balanced).value_counts())\n",
                "print(\"\\n‚úÖ Classes perfectly balanced for training!\")\n",
                "\n",
                "# ‚úÖ ASSERTION: Verify balancing worked\n",
                "assert X_train_balanced.shape[0] == y_train_balanced.shape[0], \"Balanced data shape mismatch!\"\n",
                "print(\"‚úÖ Balance verification passed!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a id='section4'></a>\n",
                "## 4. Model Training & Comparison\n",
                "\n",
                "### Why These Two Models?\n",
                "\n",
                "**Naive Bayes**:\n",
                "- Fast and simple\n",
                "- Works well with smaller datasets\n",
                "- Assumes feature independence (which may not always be true)\n",
                "\n",
                "**Logistic Regression**:\n",
                "- Interpretable coefficients (feature importance)\n",
                "- No independence assumption\n",
                "- Industry standard for binary classification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train Naive Bayes\n",
                "print(\"ü§ñ Training Naive Bayes Model...\")\n",
                "nb_model = GaussianNB()\n",
                "nb_model.fit(X_train_balanced, y_train_balanced)\n",
                "print(\"‚úÖ Naive Bayes trained!\\n\")\n",
                "\n",
                "# Train Logistic Regression\n",
                "print(\"ü§ñ Training Logistic Regression Model...\")\n",
                "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
                "lr_model.fit(X_train_balanced, y_train_balanced)\n",
                "print(\"‚úÖ Logistic Regression trained!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Make predictions\n",
                "nb_pred = nb_model.predict(X_test_processed)\n",
                "lr_pred = lr_model.predict(X_test_processed)\n",
                "\n",
                "# Calculate metrics\n",
                "nb_metrics = {\n",
                "    'Model': 'Naive Bayes',\n",
                "    'Accuracy': accuracy_score(y_test, nb_pred),\n",
                "    'Precision': precision_score(y_test, nb_pred, zero_division=0),\n",
                "    'Recall': recall_score(y_test, nb_pred, zero_division=0),\n",
                "    'F1-Score': f1_score(y_test, nb_pred, zero_division=0)\n",
                "}\n",
                "\n",
                "lr_metrics = {\n",
                "    'Model': 'Logistic Regression',\n",
                "    'Accuracy': accuracy_score(y_test, lr_pred),\n",
                "    'Precision': precision_score(y_test, lr_pred, zero_division=0),\n",
                "    'Recall': recall_score(y_test, lr_pred, zero_division=0),\n",
                "    'F1-Score': f1_score(y_test, lr_pred, zero_division=0)\n",
                "}\n",
                "\n",
                "# Create comparison DataFrame\n",
                "comparison_df = pd.DataFrame([nb_metrics, lr_metrics])\n",
                "comparison_df = comparison_df.set_index('Model')\n",
                "\n",
                "print(\"\\nüìä MODEL COMPARISON RESULTS\")\n",
                "print(\"=\" * 70)\n",
                "print(comparison_df.round(4))\n",
                "print(\"=\" * 70)\n",
                "\n",
                "# Determine winner\n",
                "winner = comparison_df['F1-Score'].idxmax()\n",
                "print(f\"\\nüèÜ Winner (by F1-Score): {winner}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize comparison\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Bar chart comparison\n",
                "comparison_df.T.plot(kind='bar', ax=axes[0], color=['#3498db', '#e74c3c'])\n",
                "axes[0].set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
                "axes[0].set_ylabel('Score', fontsize=12)\n",
                "axes[0].set_xlabel('Metric', fontsize=12)\n",
                "axes[0].legend(title='Model', loc='lower right')\n",
                "axes[0].grid(alpha=0.3)\n",
                "axes[0].set_ylim([0, 1])\n",
                "\n",
                "# Radar chart\n",
                "categories = list(comparison_df.columns)\n",
                "nb_values = comparison_df.loc['Naive Bayes'].values.tolist()\n",
                "lr_values = comparison_df.loc['Logistic Regression'].values.tolist()\n",
                "\n",
                "angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n",
                "nb_values += nb_values[:1]\n",
                "lr_values += lr_values[:1]\n",
                "angles += angles[:1]\n",
                "\n",
                "ax = plt.subplot(122, projection='polar')\n",
                "ax.plot(angles, nb_values, 'o-', linewidth=2, label='Naive Bayes', color='#3498db')\n",
                "ax.fill(angles, nb_values, alpha=0.25, color='#3498db')\n",
                "ax.plot(angles, lr_values, 'o-', linewidth=2, label='Logistic Regression', color='#e74c3c')\n",
                "ax.fill(angles, lr_values, alpha=0.25, color='#e74c3c')\n",
                "ax.set_xticks(angles[:-1])\n",
                "ax.set_xticklabels(categories)\n",
                "ax.set_ylim(0, 1)\n",
                "ax.set_title('Performance Radar Chart', fontsize=14, fontweight='bold', pad=20)\n",
                "ax.legend(loc='upper right')\n",
                "ax.grid(True)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a id='section5'></a>\n",
                "## 5. Advanced Feature Analysis\n",
                "\n",
                "### Feature Importance: Which Factors Drive Default Risk?\n",
                "\n",
                "Understanding feature importance helps us:\n",
                "- **Interpret model decisions** (regulatory compliance)\n",
                "- **Focus on critical risk factors** (business strategy)\n",
                "- **Simplify models** (remove irrelevant features)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract feature names after preprocessing\n",
                "feature_names = numerical_features.copy()\n",
                "\n",
                "# Add one-hot encoded categorical feature names\n",
                "if categorical_features:\n",
                "    encoder = preprocessor.named_transformers_['cat']\n",
                "    cat_feature_names = encoder.get_feature_names_out(categorical_features)\n",
                "    feature_names.extend(cat_feature_names)\n",
                "\n",
                "# Get Logistic Regression coefficients\n",
                "coefficients = lr_model.coef_[0]\n",
                "\n",
                "# Create feature importance DataFrame\n",
                "feature_importance = pd.DataFrame({\n",
                "    'Feature': feature_names,\n",
                "    'Coefficient': coefficients,\n",
                "    'Abs_Coefficient': np.abs(coefficients)\n",
                "}).sort_values('Abs_Coefficient', ascending=False).head(15)\n",
                "\n",
                "print(\"üîç Top 15 Most Important Features (Logistic Regression):\")\n",
                "print(feature_importance[['Feature', 'Coefficient']].to_string(index=False))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize feature importance\n",
                "plt.figure(figsize=(12, 8))\n",
                "colors = ['#e74c3c' if x < 0 else '#2ecc71' for x in feature_importance['Coefficient']]\n",
                "plt.barh(feature_importance['Feature'], feature_importance['Coefficient'], color=colors)\n",
                "plt.xlabel('Coefficient Value', fontsize=12, fontweight='bold')\n",
                "plt.ylabel('Feature', fontsize=12, fontweight='bold')\n",
                "plt.title('Feature Importance: Logistic Regression Coefficients\\n(Red = Increases Default Risk, Green = Decreases Default Risk)',\n",
                "          fontsize=14, fontweight='bold', pad=20)\n",
                "plt.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
                "plt.grid(alpha=0.3, axis='x')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nüí° Interpretation:\")\n",
                "print(\"- Positive coefficients ‚Üí Higher values INCREASE default probability\")\n",
                "print(\"- Negative coefficients ‚Üí Higher values DECREASE default probability\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üìà Interactive ROC Curve (Plotly)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate ROC curves\n",
                "nb_fpr, nb_tpr, _ = roc_curve(y_test, nb_model.predict_proba(X_test_processed)[:, 1])\n",
                "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_model.predict_proba(X_test_processed)[:, 1])\n",
                "\n",
                "nb_auc = auc(nb_fpr, nb_tpr)\n",
                "lr_auc = auc(lr_fpr, lr_tpr)\n",
                "\n",
                "if PLOTLY_AVAILABLE:\n",
                "    # Interactive Plotly ROC curve\n",
                "    fig = go.Figure()\n",
                "    \n",
                "    fig.add_trace(go.Scatter(\n",
                "        x=nb_fpr, y=nb_tpr,\n",
                "        mode='lines',\n",
                "        name=f'Naive Bayes (AUC = {nb_auc:.4f})',\n",
                "        line=dict(color='#3498db', width=3)\n",
                "    ))\n",
                "    \n",
                "    fig.add_trace(go.Scatter(\n",
                "        x=lr_fpr, y=lr_tpr,\n",
                "        mode='lines',\n",
                "        name=f'Logistic Regression (AUC = {lr_auc:.4f})',\n",
                "        line=dict(color='#e74c3c', width=3)\n",
                "    ))\n",
                "    \n",
                "    fig.add_trace(go.Scatter(\n",
                "        x=[0, 1], y=[0, 1],\n",
                "        mode='lines',\n",
                "        name='Random Classifier',\n",
                "        line=dict(color='black', width=2, dash='dash')\n",
                "    ))\n",
                "    \n",
                "    fig.update_layout(\n",
                "        title='Interactive ROC Curve Comparison',\n",
                "        xaxis_title='False Positive Rate',\n",
                "        yaxis_title='True Positive Rate',\n",
                "        width=800,\n",
                "        height=600,\n",
                "        hovermode='closest'\n",
                "    )\n",
                "    \n",
                "    fig.show()\n",
                "else:\n",
                "    # Fallback to matplotlib\n",
                "    plt.figure(figsize=(10, 8))\n",
                "    plt.plot(nb_fpr, nb_tpr, label=f'Naive Bayes (AUC = {nb_auc:.4f})', color='#3498db', lw=3)\n",
                "    plt.plot(lr_fpr, lr_tpr, label=f'Logistic Regression (AUC = {lr_auc:.4f})', color='#e74c3c', lw=3)\n",
                "    plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')\n",
                "    plt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
                "    plt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
                "    plt.title('ROC Curve Comparison', fontsize=14, fontweight='bold')\n",
                "    plt.legend(loc='lower right', fontsize=10)\n",
                "    plt.grid(alpha=0.3)\n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a id='section6'></a>\n",
                "## 6. Threshold Optimization\n",
                "\n",
                "### Why Not Always Use 0.5 as the Threshold?\n",
                "\n",
                "The default threshold of 0.5 may not be optimal for your business case:\n",
                "\n",
                "- **Lower threshold (e.g., 0.3)**: More defaults caught (higher recall), but more false alarms\n",
                "- **Higher threshold (e.g., 0.7)**: Fewer false alarms, but might miss actual defaults\n",
                "\n",
                "**Business Impact**:\n",
                "- False Positive (predict default, but pays) ‚Üí Lost revenue opportunity\n",
                "- False Negative (predict no default, but defaults) ‚Üí Financial loss\n",
                "\n",
                "Let's analyze how different thresholds affect performance!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get prediction probabilities for Logistic Regression\n",
                "lr_proba = lr_model.predict_proba(X_test_processed)[:, 1]\n",
                "\n",
                "# Test different thresholds\n",
                "thresholds = np.arange(0.1, 1.0, 0.05)\n",
                "threshold_results = []\n",
                "\n",
                "for threshold in thresholds:\n",
                "    y_pred_threshold = (lr_proba >= threshold).astype(int)\n",
                "    \n",
                "    threshold_results.append({\n",
                "        'Threshold': threshold,\n",
                "        'Accuracy': accuracy_score(y_test, y_pred_threshold),\n",
                "        'Precision': precision_score(y_test, y_pred_threshold, zero_division=0),\n",
                "        'Recall': recall_score(y_test, y_pred_threshold, zero_division=0),\n",
                "        'F1-Score': f1_score(y_test, y_pred_threshold, zero_division=0)\n",
                "    })\n",
                "\n",
                "threshold_df = pd.DataFrame(threshold_results)\n",
                "\n",
                "print(\"üéØ Threshold Analysis Results:\")\n",
                "print(threshold_df.head(10).to_string(index=False))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize threshold impact\n",
                "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
                "\n",
                "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
                "colors_map = ['#3498db', '#2ecc71', '#e74c3c', '#f39c12']\n",
                "\n",
                "for idx, (metric, color) in enumerate(zip(metrics, colors_map)):\n",
                "    row, col = idx // 2, idx % 2\n",
                "    axes[row, col].plot(threshold_df['Threshold'], threshold_df[metric], \n",
                "                        marker='o', linewidth=2, color=color, markersize=4)\n",
                "    axes[row, col].axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='Default (0.5)')\n",
                "    axes[row, col].set_xlabel('Threshold', fontsize=12, fontweight='bold')\n",
                "    axes[row, col].set_ylabel(metric, fontsize=12, fontweight='bold')\n",
                "    axes[row, col].set_title(f'{metric} vs Threshold', fontsize=13, fontweight='bold')\n",
                "    axes[row, col].grid(alpha=0.3)\n",
                "    axes[row, col].legend()\n",
                "    \n",
                "    # Find optimal threshold for this metric\n",
                "    optimal_idx = threshold_df[metric].idxmax()\n",
                "    optimal_threshold = threshold_df.loc[optimal_idx, 'Threshold']\n",
                "    optimal_value = threshold_df.loc[optimal_idx, metric]\n",
                "    axes[row, col].axvline(x=optimal_threshold, color='green', linestyle=':', linewidth=2, \n",
                "                           label=f'Optimal ({optimal_threshold:.2f})')\n",
                "    axes[row, col].legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nüí° Key Insights:\")\n",
                "print(f\"- Optimal threshold for F1-Score: {threshold_df.loc[threshold_df['F1-Score'].idxmax(), 'Threshold']:.2f}\")\n",
                "print(f\"- Optimal threshold for Recall: {threshold_df.loc[threshold_df['Recall'].idxmax(), 'Threshold']:.2f}\")\n",
                "print(f\"- Optimal threshold for Precision: {threshold_df.loc[threshold_df['Precision'].idxmax(), 'Threshold']:.2f}\")\n",
                "print(\"\\n‚ö†Ô∏è Choose threshold based on business priorities!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a id='section7'></a>\n",
                "## 7. Conclusions & Recommendations\n",
                "\n",
                "### üìä Summary of Findings\n",
                "\n",
                "**Model Performance**:\n",
                "- Logistic Regression outperformed Naive Bayes across all metrics\n",
                "- Best F1-Score indicates good balance between precision and recall\n",
                "- ROC curves show both models perform well above random chance\n",
                "\n",
                "**Key Risk Factors** (based on feature importance):\n",
                "- Review the top features from the coefficient analysis\n",
                "- These should inform lending policies and risk assessment\n",
                "\n",
                "**Threshold Selection**:\n",
                "- Default 0.5 may not be optimal\n",
                "- Consider business costs when choosing threshold\n",
                "- Higher threshold ‚Üí fewer loans approved but safer\n",
                "- Lower threshold ‚Üí more loans approved but riskier\n",
                "\n",
                "### üéØ Recommendations\n",
                "\n",
                "1. **Deploy Logistic Regression** as the primary model\n",
                "2. **Monitor feature importance** regularly to detect changing patterns\n",
                "3. **Adjust threshold** based on risk appetite and market conditions\n",
                "4. **Implement A/B testing** to validate model performance in production\n",
                "5. **Regular retraining** as new data becomes available\n",
                "\n",
                "### üöÄ Next Steps\n",
                "\n",
                "- Cross-validation for more robust evaluation\n",
                "- Try ensemble methods (Random Forest, XGBoost)\n",
                "- Feature engineering based on domain expertise\n",
                "- Cost-sensitive learning (assign different costs to errors)\n",
                "- Explainability tools (SHAP values) for regulatory compliance\n",
                "\n",
                "---\n",
                "\n",
                "**Thank you for following this Master Class analysis! üéì**"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}